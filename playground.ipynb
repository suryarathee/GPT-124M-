{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-05T10:00:54.522357Z",
     "start_time": "2025-04-05T10:00:45.776615Z"
    }
   },
   "source": [
    "from dataclasses import dataclass\n",
    "from datetime import time\n",
    "from itertools import repeat\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math    \n",
    "\n",
    "from sympy import false\n",
    "from torch.backends.mkl import verbose\n",
    "from torch.nn import functional as F"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T11:12:42.995425Z",
     "start_time": "2025-04-05T11:12:42.984418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.get_device_name(0))"
   ],
   "id": "6ab94a9c1e69a5c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T12:25:18.177643Z",
     "start_time": "2025-04-05T12:25:18.143371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    \"\"\"Hyper Parameters for GPT\"\"\"\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50257\n",
    "    n_layer: int = 12\n",
    "    n_embd: int = 768\n",
    "    n_head: int = 12\n",
    "    \n",
    "class CasualSelfAttention(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_layer == 0\n",
    "        # key query value projection for all heads,but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3*config.n_embd)\n",
    "        #output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        \n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        ## bias/mask following OpenAI/HF naming\n",
    "        self.register_buffer(\"bias\",torch.tril(torch.ones(config.block_size,config.block_size)).view(1,1,config.block_size,config.block_size))\n",
    "        self.NANOGPT_SCALE_INIT = 1\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        B,T,C = x.size()#batch size,sequence length,embedding  dimensionality(n_embd)\n",
    "        \n",
    "            #calculate query ,key,value for all heads in batch and move head forward to be the batch \n",
    "            #nh in \"number of heads\",hs is \"head size\" an C(number of channels) = nh*hs\n",
    "            #e.g. in GPT-2 (124),n_head = 12,hs = 64,so nh*hs=C=768 channels in Transformer \n",
    "        qkv = self.c_attn(x)\n",
    "        q,k,v = qkv.split(self.n_embd,dim = 2)\n",
    "        k = k.view(B,T,self.n_head,C//self.n_head).transpose(1,2)#(B,nh,T,hs)\n",
    "        q = q.view(B,T,self.n_head,C//self.n_head).transpose(1,2)#(B,\n",
    "        v = v.view(B,T,self.n_head,C//self.n_head).transpose(1,2)#(B,nh,T,hs)            #attention (materialize the large(T,T)matrix for all the queries and keys)\n",
    "        \n",
    "        att = (q @ k.transpose(-2,-1))*(1.0/math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T]== 0,float('-inf'))\n",
    "        att = F.softmax(att,dim=-1)\n",
    "        y = att @ v #(B,nh,T,T)x(B,nh,T,hs)\n",
    "        y = y.transpose(1,2).contiguous().view(B,T,C)\n",
    "        #output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd,4* config.n_embd)\n",
    "        self.gelu = nn.GELU(approximate='tanh')#can use approximate = none also\n",
    "        self.c_proj = nn.Linear(4*config.n_embd,config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CasualSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "    # residual network\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x+  self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "class GPT(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),#weights of tocken embedding\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),# weights of position embedding\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size,bias=False)\n",
    "        #weight sharing  scheme\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module,\"NANOGPT_SCALE_INIT\"):\n",
    "                std *= (2*self.config.n_layer) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight,mean = 0.0,std = std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight,mean = 0.0,std = 0.2)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=device) # shape (t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls,model_type):\n",
    "        \"\"\"Load pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2','gpt2-medium','gpt2-large','gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\"%model_type)\n",
    "        #n_layer,n_headand n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':dict(n_layer = 12,n_head = 12,n_embd = 768),#124M params\n",
    "            'gpt2-meadium':dict(n_layer = 24,n_head = 16,n_embd = 1024),#350M params\n",
    "            'gpt2-large':dict(n_layer = 36,n_head = 20,n_embd = 1280),\n",
    "            'gpt2-xl':dict(n_layer = 48,n_head = 25,n_embd = 1600)\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')]\n",
    "        #discard this mask\n",
    "        \n",
    "        #init a huggingface/transformer model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "        \n",
    "        #copy while ensuring all of the parameter are aligned and match in name and shape\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model"
   ],
   "id": "295e2efc24ab42c",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    " model = GPT.from_pretrained('gpt2')\n",
    " print(\"didn't crash\")"
   ],
   "id": "4338ef08b45eef37",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_return_sequence = 5\n",
    "max_length  = 30\n",
    "model.eval()\n",
    "model.to(\"cuda\")"
   ],
   "id": "909cc7998d4326e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T10:01:19.931674Z",
     "start_time": "2025-04-05T10:01:19.340710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "enc = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tockens = enc.encode(\"Hello ,I am a language model\")\n",
    "tockens = torch.tensor(tockens,dtype=torch.long)\n",
    "tockens = tockens.unsqueeze(0).repeat(num_return_sequence,1)\n",
    "#x = tockens.to(\"cuda\")"
   ],
   "id": "397c1e1c5e546e3e",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T12:19:45.338495Z",
     "start_time": "2025-04-05T12:19:45.323450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('input.txt','r') as f:\n",
    "    text = f.read()\n",
    "text = text[:1000]\n",
    "tockens = enc.encode(text)\n",
    "B, T = 4,32\n",
    "buf = torch.tensor(tockens[:B*T +1])\n",
    "x = buf[:-1].view(B,T)\n",
    "y = buf[1:].view(B,T)\n",
    "x = x.to('cuda')\n",
    "y = y.to('cuda')"
   ],
   "id": "21d3904e686c1c56",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T10:01:20.822136Z",
     "start_time": "2025-04-05T10:01:20.011216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#generate! right now x is (B,T) where B= 5,T= 8\n",
    "#set the seed to 45\n",
    "torch.manual_seed(45)\n",
    "torch.cuda.manual_seed(45)\n",
    "while x.size(1) < max_length:\n",
    "    #forward the model to get the logits\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)#(B,T,vocab_size)\n",
    "        #take the logits to last position\n",
    "        logits = logits[0]\n",
    "        logits = logits[:,-1,:]#(b,vocab_size)\n",
    "        #get the probabilities\n",
    "        probs = F.softmax(logits,dim=-1)\n",
    "        #do top-k sampeling of 50 \n",
    "        #topk_probs here becomes(5,50) \n",
    "        top_k_probs,topk_indices = torch.topk(probs,50,dim=-1)\n",
    "        #select the tocken from the top-k probabilities\n",
    "        ix = torch.multinomial(top_k_probs,1)#(B,1)\n",
    "        xcol = torch.gather(topk_indices,-1,ix)\n",
    "        x =  torch.cat((x,xcol),dim=-1)\n",
    "        \n",
    "        \n",
    "for i in range(num_return_sequence):\n",
    "    tockens = x[i,:max_length].tolist()\n",
    "    decoded = enc.decode(tockens)\n",
    "    print(\">\",decoded)"
   ],
   "id": "e4d20e3e67d1114d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Hello ,I am a language modeler. I have developed some advanced data structure and type system, but since when does type modeler actually have any\n",
      "> Hello ,I am a language modeler who works on a pretty much any language in the world.\n",
      "\n",
      "I am a language modeler who works\n",
      "> Hello ,I am a language modeler and the problem should be solving the way he tells me I should solve it.\n",
      "\n",
      "-\n",
      "\n",
      "For\n",
      "> Hello ,I am a language modeler so I will get the answers you want in my book, which I will link to when I discuss it in\n",
      "> Hello ,I am a language modeler. I'm also a writer and a developer! I hope that you may see me as something like this:\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T11:20:52.339130Z",
     "start_time": "2025-04-05T11:20:51.117140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = GPT(GPTConfig)\n",
    "model.to(\"cuda\")\n",
    "#logits,loss= model(x,y)"
   ],
   "id": "6a3a726656b1399",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.9937, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T11:29:36.348526Z",
     "start_time": "2025-04-05T11:26:13.577262Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "eabd57467665c62c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0,loss 10.993712425231934\n",
      "step 1,loss 6.602757930755615\n",
      "step 2,loss 4.26934814453125\n",
      "step 3,loss 2.629721164703369\n",
      "step 4,loss 1.5756525993347168\n",
      "step 5,loss 0.8535947799682617\n",
      "step 6,loss 0.4760079085826874\n",
      "step 7,loss 0.2819773554801941\n",
      "step 8,loss 0.18432612717151642\n",
      "step 9,loss 0.1321418434381485\n",
      "step 10,loss 0.09182970225811005\n",
      "step 11,loss 0.06126199662685394\n",
      "step 12,loss 0.05187484249472618\n",
      "step 13,loss 0.04428418353199959\n",
      "step 14,loss 0.03507862612605095\n",
      "step 15,loss 0.028502007946372032\n",
      "step 16,loss 0.025079278275370598\n",
      "step 17,loss 0.022762050852179527\n",
      "step 18,loss 0.02012510970234871\n",
      "step 19,loss 0.017332274466753006\n",
      "step 20,loss 0.01506054773926735\n",
      "step 21,loss 0.01343008503317833\n",
      "step 22,loss 0.012222801335155964\n",
      "step 23,loss 0.011225614696741104\n",
      "step 24,loss 0.010309499688446522\n",
      "step 25,loss 0.00943364854902029\n",
      "step 26,loss 0.008617345243692398\n",
      "step 27,loss 0.00788948219269514\n",
      "step 28,loss 0.007259665988385677\n",
      "step 29,loss 0.0067200916819274426\n",
      "step 30,loss 0.006257147993892431\n",
      "step 31,loss 0.005857507698237896\n",
      "step 32,loss 0.0055098035372793674\n",
      "step 33,loss 0.0052044750191271305\n",
      "step 34,loss 0.004934005439281464\n",
      "step 35,loss 0.004692455753684044\n",
      "step 36,loss 0.004475377034395933\n",
      "step 37,loss 0.004279169719666243\n",
      "step 38,loss 0.004101211670786142\n",
      "step 39,loss 0.003939339891076088\n",
      "step 40,loss 0.00379178486764431\n",
      "step 41,loss 0.0036570653319358826\n",
      "step 42,loss 0.0035337654408067465\n",
      "step 43,loss 0.0034206383861601353\n",
      "step 44,loss 0.0033166443463414907\n",
      "step 45,loss 0.003220716491341591\n",
      "step 46,loss 0.003131960751488805\n",
      "step 47,loss 0.0030496197286993265\n",
      "step 48,loss 0.0029729288071393967\n",
      "step 49,loss 0.002901326399296522\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T13:14:35.709401Z",
     "start_time": "2025-04-05T13:14:35.701400Z"
    }
   },
   "cell_type": "code",
   "source": "import time",
   "id": "ae5cbed0456b1ccf",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T12:19:53.784402Z",
     "start_time": "2025-04-05T12:19:53.771402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DataLoaderLite:\n",
    "    def __init__(self,B,T):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        \n",
    "        #at init load tokens from disk and store them in memory\n",
    "        with open('input.txt','r') as f:\n",
    "            text = f.read()\n",
    "        enc = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        tockens = enc.encode(text)\n",
    "        self.tockens = torch.tensor(tockens)\n",
    "        print(f\"loaded{len(self.tockens)} tokens\")\n",
    "        print(f'1 epoch = {len(self.tockens)//(B*T)} batches')\n",
    "        \n",
    "        self.current_position = 0\n",
    "    \n",
    "    def next_batch(self):\n",
    "        B,T = self.B,self.T\n",
    "        buf = self.tockens[self.current_position:self.current_position+B*T+1]\n",
    "        x = buf[:-1].view(B,T) # imputs\n",
    "        y = buf[1:].view(B,T) #targets\n",
    "        x = x.to('cuda')\n",
    "        y = y.to('cuda')\n",
    "        self.current_position += B*T\n",
    "        if self.current_position +  (B*T+1) >= len(self.tockens):\n",
    "            self.current_position = 0\n",
    "        return x,y"
   ],
   "id": "685e3928eb61cf68",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "train_loader = DataLoaderLite(B = 4,T = 32)\n",
    "model = GPT(GPTConfig)\n",
    "model.to(\"cuda\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr = 3e-4,betas = (0.9,0.95),eps = 1e-8)\n",
    "for i in range(50):\n",
    "    t0 = time.time()\n",
    "    x,y = train_loader.next_batch()\n",
    "    x,y = x.to('cuda'),y.to('cuda')\n",
    "    optimizer.zero_grad()\n",
    "    logits,loss = model(x,y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    dt = (t1-t0)*1000\n",
    "    print(f\"step {i},loss {loss.item()},dt:{dt:.2f}ms\")"
   ],
   "id": "f104b3e11b6bec1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model.eval()\n",
    "num_return_sequence = 5\n",
    "max_length = 30\n",
    "tokens = enc.encode(\"Hello ,I am a language model\")\n",
    "tockens = torch.tensor(tokens,dtype=torch.long)"
   ],
   "id": "aff4a0ee261057af"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-kernal",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
