{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-20T09:16:49.166961Z",
     "start_time": "2025-02-20T09:16:25.484124Z"
    }
   },
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.nn import functional as F"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from scipy.stats import binom\n",
    "prob = binom.cdf(9, 200, 0.05)\n",
    "print(prob)"
   ],
   "id": "6f97345523ea51e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    \"\"\"Hyper Parameters for GPT\"\"\"\n",
    "    block_size: int = 254\n",
    "    vocab_size: int = 65\n",
    "    n_layer: int = 6\n",
    "    n_embd: int = 384\n",
    "    n_head: int = 8\n",
    "\n",
    "class CasualSelfAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_layer == 0\n",
    "        # key query value projection for all heads,but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3*config.n_embd)\n",
    "        #output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        \n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        ## bias/mask following OpenAI/HF naming\n",
    "        self.register_buffer(\"bias\",torch.tril(torch.ones(config.block_size,config.block_size)).view(1,1,config.block_size,config.block_size))\n",
    "        \n",
    "        def forward(self,x):\n",
    "            B,T,C = x.size()#batch size,sequence length,embedding  dimensionality(n_embd)\n",
    "            \n",
    "            #calculate query ,key,value for all heads in batch and move head forward to be the batch \n",
    "            #nh in \"number of heads\",hs is \"head size\" an C(number of channels) = nh*hs\n",
    "            #e.g. in GPT-2 (124),n_head = 12,hs = 64,so nh*hs=C=768 channels in Transformer \n",
    "            qkv = self.c_attn(x)\n",
    "            q,k,v = qkv.split(self.n_embd,dim = 2)\n",
    "            k = k.view(B,T,self.n_head,C//self.n_head).transpose(1,2)#(B,nh,T,hs)\n",
    "            q = q.view(B,T,self.n_head,C//self.n_head).transpose(1,2)#(B,\n",
    "            v = v.view(B,T,self.n_head,C//self.n_head).transpose(1,2)#(B,nh,T,hs)\n",
    "            #attention (materialize the large(T,T)matrix for all the queries and keys)\n",
    "            att = (q @ k.transpose(-2,-1))*(1.0/math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T]== 0,float('-inf'))\n",
    "            att = F.softmax(att,dim=-1)\n",
    "            y = att @ v #(B,nh,T,T)x(B,nh,T,hs)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd,4* config.n_embd)\n",
    "        self.gelu = nn.GELU(approximate='tanh')#can use approximate = none also\n",
    "        self.c_proj = nn.Linear(4*config.n_embd,config.n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = nn.CasualSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "    # residual network\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x+  self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "       \n",
    "class GPT(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),#weights of tocken embedding\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),# weights of position embedding\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size,bias=False)"
   ],
   "id": "295e2efc24ab42c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
