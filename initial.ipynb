{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-19T06:40:47.936984Z",
     "start_time": "2025-03-19T06:40:22.351417Z"
    }
   },
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.nn import functional as F"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T06:53:28.506484Z",
     "start_time": "2025-03-19T06:53:28.477377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CasualSelfAttention(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_layer == 0\n",
    "        # key query value projection for all heads,but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3*config.n_embd)\n",
    "        #output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        \n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        ## bias/mask following OpenAI/HF naming\n",
    "        self.register_buffer(\"bias\",torch.tril(torch.ones(config.block_size,config.block_size)).view(1,1,config.block_size,config.block_size))\n",
    "        \n",
    "        def forward(self,x):\n",
    "            B,T,C = x.size()#batch size,sequence length,embedding  dimensionality(n_embd)\n",
    "            \n",
    "            #calculate query ,key,value for all heads in batch and move head forward to be the batch \n",
    "            #nh in \"number of heads\",hs is \"head size\" an C(number of channels) = nh*hs\n",
    "            #e.g. in GPT-2 (124),n_head = 12,hs = 64,so nh*hs=C=768 channels in Transformer \n",
    "            qkv = self.c_attn(x)\n",
    "            q,k,v = qkv.split(self.n_embd,dim = 2)\n",
    "            k = k.view(B,T,self.n_head,C//self.n_head).transpose(1,2)#(B,nh,T,hs)\n",
    "            q = q.view(B,T,self.n_head,C//self.n_head).transpose(1,2)#(B,\n",
    "            v = v.view(B,T,self.n_head,C//self.n_head).transpose(1,2)#(B,nh,T,hs)\n",
    "            #attention (materialize the large(T,T)matrix for all the queries and keys)\n",
    "            att = (q @ k.transpose(-2,-1))*(1.0/math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T]== 0,float('-inf'))\n",
    "            att = F.softmax(att,dim=-1)\n",
    "            y = att @ v #(B,nh,T,T)x(B,nh,T,hs)\n",
    "            y = y.transpose(1,2).contiguous().view(B,T,C)\n",
    "            #output projection\n",
    "            y = self.c_proj(y)\n",
    "            return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd,4* config.n_embd)\n",
    "        self.gelu = nn.GELU(approximate='tanh')#can use approximate = none also\n",
    "        self.c_proj = nn.Linear(4*config.n_embd,config.n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CasualSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "    # residual network\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x+  self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    \"\"\"Hyper Parameters for GPT\"\"\"\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50257\n",
    "    n_layer: int = 12\n",
    "    n_embd: int = 768\n",
    "    n_head: int = 12\n",
    "\n",
    "   \n",
    "class GPT(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),#weights of tocken embedding\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),# weights of position embedding\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size,bias=False)\n",
    "    @classmethod\n",
    "    def from_pretrained(cls,model_type):\n",
    "        \"\"\"Load pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2','gpt2-medium','gpt2-large','gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\"%model_type)\n",
    "        #n_layer,n_headand n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':dict(n_layer = 12,n_head = 12,n_embd = 768),#124M params\n",
    "            'gpt2-meadium':dict(n_layer = 24,n_head = 16,n_embd = 1024),#350M params\n",
    "            'gpt2-large':dict(n_layer = 36,n_head = 20,n_embd = 1280),\n",
    "            'gpt2-xl':dict(n_layer = 48,n_head = 25,n_embd = 1600)\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')]\n",
    "        #discard this mask\n",
    "        \n",
    "        #init a huggingface/transformer model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "        \n",
    "        #copy while ensuring all of the parameter are aligned and match in name and shape\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model"
   ],
   "id": "295e2efc24ab42c",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T06:53:31.728548Z",
     "start_time": "2025-03-19T06:53:29.309128Z"
    }
   },
   "cell_type": "code",
   "source": [
    " model = GPT.from_pretrained('gpt2')\n",
    " print(\"didn't crash\")"
   ],
   "id": "4338ef08b45eef37",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n",
      "didn't crash\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "909cc7998d4326e8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
