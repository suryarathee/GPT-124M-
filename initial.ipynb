{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-05T10:00:54.522357Z",
     "start_time": "2025-04-05T10:00:45.776615Z"
    }
   },
   "source": [
    "from dataclasses import dataclass\n",
    "from itertools import repeat\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.nn import functional as F"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T10:00:54.602015Z",
     "start_time": "2025-04-05T10:00:54.547391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.get_device_name(0))"
   ],
   "id": "6ab94a9c1e69a5c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T10:00:54.960861Z",
     "start_time": "2025-04-05T10:00:54.931455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    \"\"\"Hyper Parameters for GPT\"\"\"\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50257\n",
    "    n_layer: int = 12\n",
    "    n_embd: int = 768\n",
    "    n_head: int = 12\n",
    "    \n",
    "class CasualSelfAttention(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_layer == 0\n",
    "        # key query value projection for all heads,but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3*config.n_embd)\n",
    "        #output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        \n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        ## bias/mask following OpenAI/HF naming\n",
    "        self.register_buffer(\"bias\",torch.tril(torch.ones(config.block_size,config.block_size)).view(1,1,config.block_size,config.block_size))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        B,T,C = x.size()#batch size,sequence length,embedding  dimensionality(n_embd)\n",
    "        \n",
    "            #calculate query ,key,value for all heads in batch and move head forward to be the batch \n",
    "            #nh in \"number of heads\",hs is \"head size\" an C(number of channels) = nh*hs\n",
    "            #e.g. in GPT-2 (124),n_head = 12,hs = 64,so nh*hs=C=768 channels in Transformer \n",
    "        qkv = self.c_attn(x)\n",
    "        q,k,v = qkv.split(self.n_embd,dim = 2)\n",
    "        k = k.view(B,T,self.n_head,C//self.n_head).transpose(1,2)#(B,nh,T,hs)\n",
    "        q = q.view(B,T,self.n_head,C//self.n_head).transpose(1,2)#(B,\n",
    "        v = v.view(B,T,self.n_head,C//self.n_head).transpose(1,2)#(B,nh,T,hs)            #attention (materialize the large(T,T)matrix for all the queries and keys)\n",
    "        \n",
    "        att = (q @ k.transpose(-2,-1))*(1.0/math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T]== 0,float('-inf'))\n",
    "        att = F.softmax(att,dim=-1)\n",
    "        y = att @ v #(B,nh,T,T)x(B,nh,T,hs)\n",
    "        y = y.transpose(1,2).contiguous().view(B,T,C)\n",
    "        #output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd,4* config.n_embd)\n",
    "        self.gelu = nn.GELU(approximate='tanh')#can use approximate = none also\n",
    "        self.c_proj = nn.Linear(4*config.n_embd,config.n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CasualSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "    # residual network\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x+  self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "class GPT(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),#weights of tocken embedding\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),# weights of position embedding\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size,bias=False)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=device) # shape (t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls,model_type):\n",
    "        \"\"\"Load pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2','gpt2-medium','gpt2-large','gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\"%model_type)\n",
    "        #n_layer,n_headand n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':dict(n_layer = 12,n_head = 12,n_embd = 768),#124M params\n",
    "            'gpt2-meadium':dict(n_layer = 24,n_head = 16,n_embd = 1024),#350M params\n",
    "            'gpt2-large':dict(n_layer = 36,n_head = 20,n_embd = 1280),\n",
    "            'gpt2-xl':dict(n_layer = 48,n_head = 25,n_embd = 1600)\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')]\n",
    "        #discard this mask\n",
    "        \n",
    "        #init a huggingface/transformer model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "        \n",
    "        #copy while ensuring all of the parameter are aligned and match in name and shape\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model"
   ],
   "id": "295e2efc24ab42c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T10:01:19.009278Z",
     "start_time": "2025-04-05T10:00:55.093429Z"
    }
   },
   "cell_type": "code",
   "source": [
    " model = GPT.from_pretrained('gpt2')\n",
    " print(\"didn't crash\")"
   ],
   "id": "4338ef08b45eef37",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rathe\\.conda\\envs\\newconda\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "loading weights from pretrained gpt: gpt2\n",
      "didn't crash\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T10:01:19.322174Z",
     "start_time": "2025-04-05T10:01:19.058464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_return_sequence = 5\n",
    "max_length  = 30\n",
    "model.eval()\n",
    "model.to(\"cuda\")"
   ],
   "id": "909cc7998d4326e8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CasualSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9ab190117e2f3b83"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T10:01:19.931674Z",
     "start_time": "2025-04-05T10:01:19.340710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "enc = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tockens = enc.encode(\"Hello ,I am a language model\")\n",
    "tockens = torch.tensor(tockens,dtype=torch.long)\n",
    "tockens = tockens.unsqueeze(0).repeat(num_return_sequence,1)\n",
    "x = tockens.to(\"cuda\")"
   ],
   "id": "397c1e1c5e546e3e",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T10:01:20.822136Z",
     "start_time": "2025-04-05T10:01:20.011216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#generate! right now x is (B,T) where B= 5,T= 8\n",
    "#set the seed to 45\n",
    "torch.manual_seed(45)\n",
    "torch.cuda.manual_seed(45)\n",
    "while x.size(1) < max_length:\n",
    "    #forward the model to get the logits\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)#(B,T,vocab_size)\n",
    "        #take the logits to last position\n",
    "        logits = logits[0]\n",
    "        logits = logits[:,-1,:]#(b,vocab_size)\n",
    "        #get the probabilities\n",
    "        probs = F.softmax(logits,dim=-1)\n",
    "        #do top-k sampeling of 50 \n",
    "        #topk_probs here becomes(5,50) \n",
    "        top_k_probs,topk_indices = torch.topk(probs,50,dim=-1)\n",
    "        #select the tocken from the top-k probabilities\n",
    "        ix = torch.multinomial(top_k_probs,1)#(B,1)\n",
    "        xcol = torch.gather(topk_indices,-1,ix)\n",
    "        x =  torch.cat((x,xcol),dim=-1)\n",
    "        \n",
    "        \n",
    "for i in range(num_return_sequence):\n",
    "    tockens = x[i,:max_length].tolist()\n",
    "    decoded = enc.decode(tockens)\n",
    "    print(\">\",decoded)"
   ],
   "id": "e4d20e3e67d1114d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Hello ,I am a language modeler. I have developed some advanced data structure and type system, but since when does type modeler actually have any\n",
      "> Hello ,I am a language modeler who works on a pretty much any language in the world.\n",
      "\n",
      "I am a language modeler who works\n",
      "> Hello ,I am a language modeler and the problem should be solving the way he tells me I should solve it.\n",
      "\n",
      "-\n",
      "\n",
      "For\n",
      "> Hello ,I am a language modeler so I will get the answers you want in my book, which I will link to when I discuss it in\n",
      "> Hello ,I am a language modeler. I'm also a writer and a developer! I hope that you may see me as something like this:\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T10:01:20.961738Z",
     "start_time": "2025-04-05T10:01:20.948196Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6a3a726656b1399",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-kernal",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
